1
00:00:00,000 --> 00:00:02,220
hello guys welcome to my video about the

2
00:00:02,220 --> 00:00:04,680
Transformer and this is actually the

3
00:00:04,680 --> 00:00:07,620
person 2.0 of my series on the

4
00:00:07,620 --> 00:00:10,620
Transformer I had a previous video in

5
00:00:10,620 --> 00:00:12,420
which I talked about the Transformer but

6
00:00:12,420 --> 00:00:14,280
the audio quality was not good and as

7
00:00:14,280 --> 00:00:16,740
suggested by my viewers as the video was

8
00:00:16,740 --> 00:00:19,859
really uh had a huge success the viewers

9
00:00:19,859 --> 00:00:21,720
suggested me to to improve their audio

10
00:00:21,720 --> 00:00:23,520
quality so this this is why I'm doing

11
00:00:23,520 --> 00:00:24,539
this video

12
00:00:24,539 --> 00:00:26,460
uh you don't have to watch the previous

13
00:00:26,460 --> 00:00:27,840
series because I would be doing

14
00:00:27,840 --> 00:00:29,640
basically the same things but with some

15
00:00:29,640 --> 00:00:30,840
improvements so I'm actually

16
00:00:30,840 --> 00:00:33,180
compensating from some mistakes I made

17
00:00:33,180 --> 00:00:34,980
or from some improvements that I could

18
00:00:34,980 --> 00:00:35,940
add

19
00:00:35,940 --> 00:00:37,860
after watching this video I suggest

20
00:00:37,860 --> 00:00:40,200
watch my watching my other video about

21
00:00:40,200 --> 00:00:43,020
or how to code a Transformer model from

22
00:00:43,020 --> 00:00:45,480
scratch so how to code the model itself

23
00:00:45,480 --> 00:00:48,300
how to train it online data and how to

24
00:00:48,300 --> 00:00:50,520
inference it stick it with me because

25
00:00:50,520 --> 00:00:52,620
it's gonna be a little long journey but

26
00:00:52,620 --> 00:00:54,239
for sure what

27
00:00:54,239 --> 00:00:56,940
now before we talk about the Transformer

28
00:00:56,940 --> 00:00:59,280
I want to first talk about recurrent

29
00:00:59,280 --> 00:01:02,039
neural networks so the networks that

30
00:01:02,039 --> 00:01:03,899
were used before they introduced the

31
00:01:03,899 --> 00:01:06,299
transformer for most of the sequence to

32
00:01:06,299 --> 00:01:11,220
sequence jobs tasks so let's review them

33
00:01:11,220 --> 00:01:14,580
recurring neural networks existed a long

34
00:01:14,580 --> 00:01:16,979
time before the Transformer and they

35
00:01:16,979 --> 00:01:19,920
allowed to map one sequence of input to

36
00:01:19,920 --> 00:01:22,080
another sequence of output in this case

37
00:01:22,080 --> 00:01:25,200
our input is X and we want an input

38
00:01:25,200 --> 00:01:29,040
sequence Y what we did before is that we

39
00:01:29,040 --> 00:01:31,680
split the sequence into single items so

40
00:01:31,680 --> 00:01:34,020
we gave the recurrent neural network the

41
00:01:34,020 --> 00:01:37,799
first item as input so X1 along with an

42
00:01:37,799 --> 00:01:40,200
initial State usually made up of only

43
00:01:40,200 --> 00:01:42,960
zeros and the recurrent normal Network

44
00:01:42,960 --> 00:01:46,439
produced an output let's call it y1

45
00:01:46,439 --> 00:01:49,680
and this happened at the first time step

46
00:01:49,680 --> 00:01:53,220
then we took the hidden State this is

47
00:01:53,220 --> 00:01:54,840
called the hidden state of the network

48
00:01:54,840 --> 00:01:57,960
of the previous time step along with the

49
00:01:57,960 --> 00:02:01,320
next input token so X2 and the network

50
00:02:01,320 --> 00:02:04,799
had to produce the SEC the second output

51
00:02:04,799 --> 00:02:08,639
token Y2 and then we did it the same

52
00:02:08,639 --> 00:02:10,500
procedure at the third time step in

53
00:02:10,500 --> 00:02:12,420
which we took the hidden state of the

54
00:02:12,420 --> 00:02:14,879
previous time step along with the input

55
00:02:14,879 --> 00:02:18,300
State the input token at the time steps

56
00:02:18,300 --> 00:02:21,180
3 and the network has to produce the

57
00:02:21,180 --> 00:02:24,239
next output token which is Y3 if you

58
00:02:24,239 --> 00:02:27,239
have enter n tokens you need n time

59
00:02:27,239 --> 00:02:30,840
steps to map a end sequence input into

60
00:02:30,840 --> 00:02:33,260
an end sequence output

61
00:02:33,260 --> 00:02:36,660
this worked fine for a lot of tasks but

62
00:02:36,660 --> 00:02:40,640
had some problems let's review them

63
00:02:40,739 --> 00:02:42,599
the problems with recurring neural

64
00:02:42,599 --> 00:02:45,060
networks first of all are that they are

65
00:02:45,060 --> 00:02:48,239
slow for long sequences because think of

66
00:02:48,239 --> 00:02:50,879
the process we did before we have kind

67
00:02:50,879 --> 00:02:53,580
of like a for Loop in which we do the

68
00:02:53,580 --> 00:02:56,519
same operation for every token in the

69
00:02:56,519 --> 00:02:58,920
input so if you have the longer the

70
00:02:58,920 --> 00:03:02,459
sequence the longer this computation and

71
00:03:02,459 --> 00:03:05,640
this made the the network not easy to

72
00:03:05,640 --> 00:03:07,800
train for long sequences the second

73
00:03:07,800 --> 00:03:09,840
problem was the vanishing or the

74
00:03:09,840 --> 00:03:12,300
exploding gradients now you may have

75
00:03:12,300 --> 00:03:14,459
heard these terms or expression on the

76
00:03:14,459 --> 00:03:16,980
Internet or from other videos but I will

77
00:03:16,980 --> 00:03:20,400
try to give you a brief Insight on what

78
00:03:20,400 --> 00:03:22,319
does what do they mean on a practical

79
00:03:22,319 --> 00:03:24,980
level so as you know

80
00:03:24,980 --> 00:03:27,840
Frameworks like Pi torch they convert

81
00:03:27,840 --> 00:03:31,680
our networks into a computation graph so

82
00:03:31,680 --> 00:03:33,900
basically suppose we have a computation

83
00:03:33,900 --> 00:03:36,720
graph I this is not an error network I

84
00:03:36,720 --> 00:03:38,280
will making I will be making a

85
00:03:38,280 --> 00:03:40,739
computational graph that is very simple

86
00:03:40,739 --> 00:03:42,299
has nothing to do with the neural

87
00:03:42,299 --> 00:03:44,099
networks but will show you the problems

88
00:03:44,099 --> 00:03:46,319
that we have so imagine we have two

89
00:03:46,319 --> 00:03:49,379
inputs X and another input let's call it

90
00:03:49,379 --> 00:03:51,780
y

91
00:03:51,780 --> 00:03:54,360
our computational graph first let's say

92
00:03:54,360 --> 00:03:56,580
multiplies these two numbers so we have

93
00:03:56,580 --> 00:04:00,239
a first a function let's call it f of x

94
00:04:00,239 --> 00:04:02,459
and y

95
00:04:02,459 --> 00:04:05,700
that is X multiplied by y

96
00:04:05,700 --> 00:04:09,360
let me multiplied and the result let's

97
00:04:09,360 --> 00:04:12,120
call it Z

98
00:04:12,120 --> 00:04:14,459
is map is given to another function

99
00:04:14,459 --> 00:04:18,899
let's call this function G of Z is equal

100
00:04:18,899 --> 00:04:22,320
to let's say Z squared

101
00:04:22,320 --> 00:04:25,800
what our phytorch for example does it's

102
00:04:25,800 --> 00:04:28,740
that pytorch want to calculate the

103
00:04:28,740 --> 00:04:30,900
usually we have a loss function by torch

104
00:04:30,900 --> 00:04:33,000
calculates the derivative of the loss

105
00:04:33,000 --> 00:04:35,160
function with respects to its each

106
00:04:35,160 --> 00:04:37,199
weight in this case we just calculate

107
00:04:37,199 --> 00:04:39,720
the derivative of the G function so the

108
00:04:39,720 --> 00:04:41,580
output function with respect to all of

109
00:04:41,580 --> 00:04:45,300
its inputs so derivative of G

110
00:04:45,300 --> 00:04:49,199
with respect to X let's say is equal to

111
00:04:49,199 --> 00:04:54,979
the derivative of G with respect to f

112
00:04:55,680 --> 00:04:59,220
and multiplied by the derivative of f

113
00:04:59,220 --> 00:05:02,520
with respect to X

114
00:05:02,520 --> 00:05:05,280
these two should kind of cancel out this

115
00:05:05,280 --> 00:05:08,220
is called the chain Rule now as you can

116
00:05:08,220 --> 00:05:09,240
see

117
00:05:09,240 --> 00:05:12,360
the longer the chain of computation so

118
00:05:12,360 --> 00:05:14,060
if we have many nodes one after another

119
00:05:14,060 --> 00:05:17,100
the longer this multiplication chain so

120
00:05:17,100 --> 00:05:19,860
here we have two because the distance

121
00:05:19,860 --> 00:05:22,139
from this node and this is two but

122
00:05:22,139 --> 00:05:25,919
imagine you have 100 or 1000

123
00:05:25,919 --> 00:05:30,600
now imagine this number is 0.5 and this

124
00:05:30,600 --> 00:05:34,139
number is 0.5 also the resulting numbers

125
00:05:34,139 --> 00:05:36,240
when multiplied together is a number

126
00:05:36,240 --> 00:05:38,639
that is smaller than the two initial

127
00:05:38,639 --> 00:05:41,759
numbers it's gone up 0.25 because it's

128
00:05:41,759 --> 00:05:44,220
one to one half multiplied by one half

129
00:05:44,220 --> 00:05:46,500
is one fourth

130
00:05:46,500 --> 00:05:48,539
so if we have two numbers that are

131
00:05:48,539 --> 00:05:50,639
smaller than one and we multiply them

132
00:05:50,639 --> 00:05:52,320
together they will produce an even

133
00:05:52,320 --> 00:05:54,120
smaller number and if we have two

134
00:05:54,120 --> 00:05:55,800
numbers that are bigger than one and we

135
00:05:55,800 --> 00:05:58,199
multiply them together they will produce

136
00:05:58,199 --> 00:05:59,820
a number that is bigger than both of

137
00:05:59,820 --> 00:06:02,580
them so if we have a very long chain of

138
00:06:02,580 --> 00:06:05,160
computation it eventually will either

139
00:06:05,160 --> 00:06:07,560
become a very big number or a very small

140
00:06:07,560 --> 00:06:08,400
number

141
00:06:08,400 --> 00:06:11,220
and this is not desirable first of all

142
00:06:11,220 --> 00:06:14,720
because our CPU of our GPU can only

143
00:06:14,720 --> 00:06:17,220
represent numbers up to a certain

144
00:06:17,220 --> 00:06:20,699
Precision let's say 32-bit or 64-bit and

145
00:06:20,699 --> 00:06:23,039
if the number becomes too small the

146
00:06:23,039 --> 00:06:24,900
contribution of this number to the

147
00:06:24,900 --> 00:06:27,360
output will become very small so when

148
00:06:27,360 --> 00:06:30,900
the pi torch or our automatic let's say

149
00:06:30,900 --> 00:06:33,900
our framework will calculate how to

150
00:06:33,900 --> 00:06:37,380
adjust the weights the weight will move

151
00:06:37,380 --> 00:06:39,600
very very very slowly because the

152
00:06:39,600 --> 00:06:42,660
contribution of this product is will be

153
00:06:42,660 --> 00:06:44,340
a very small number

154
00:06:44,340 --> 00:06:48,180
and this means that we have the gradient

155
00:06:48,180 --> 00:06:51,000
is Vanishing or in the other case it can

156
00:06:51,000 --> 00:06:53,880
explode become very big numbers

157
00:06:53,880 --> 00:06:55,740
and this is a problem the next problem

158
00:06:55,740 --> 00:06:57,780
is difficulty in accessing information

159
00:06:57,780 --> 00:06:59,699
from long time ago

160
00:06:59,699 --> 00:07:02,100
what does it mean it means that as you

161
00:07:02,100 --> 00:07:03,840
remember from the previous slide we saw

162
00:07:03,840 --> 00:07:06,900
that the first input token is given to

163
00:07:06,900 --> 00:07:08,699
the recurrent neural network to with

164
00:07:08,699 --> 00:07:10,380
along with the first state

165
00:07:10,380 --> 00:07:12,300
now we need to think that the recurrent

166
00:07:12,300 --> 00:07:13,800
neural network is a long graph of

167
00:07:13,800 --> 00:07:16,380
computation it will produce a new hidden

168
00:07:16,380 --> 00:07:19,680
State then we will use the the new

169
00:07:19,680 --> 00:07:21,419
hidden State along with the next token

170
00:07:21,419 --> 00:07:23,759
to produce the next output if we have a

171
00:07:23,759 --> 00:07:25,259
very long sequence

172
00:07:25,259 --> 00:07:29,580
um of input sequence the last token will

173
00:07:29,580 --> 00:07:32,460
have a hidden state whose contribution

174
00:07:32,460 --> 00:07:34,560
from the first token has nearly gone

175
00:07:34,560 --> 00:07:36,120
because of this long chain of

176
00:07:36,120 --> 00:07:38,880
multiplication so actually the last

177
00:07:38,880 --> 00:07:42,599
token will not depend much on the first

178
00:07:42,599 --> 00:07:44,819
token and this is also not good because

179
00:07:44,819 --> 00:07:48,300
for example we know as humans that in a

180
00:07:48,300 --> 00:07:50,639
text in a quite long text the context

181
00:07:50,639 --> 00:07:53,539
that we saw let's say 200 words before

182
00:07:53,539 --> 00:07:56,400
still relevant to the context of the

183
00:07:56,400 --> 00:07:59,460
current words and this is something that

184
00:07:59,460 --> 00:08:02,400
the RNN could not map

185
00:08:02,400 --> 00:08:05,639
and this is why we have the Transformer

186
00:08:05,639 --> 00:08:07,860
so the Transformer solves these problems

187
00:08:07,860 --> 00:08:09,720
with the recurrent neural networks and

188
00:08:09,720 --> 00:08:11,460
we will see how

189
00:08:11,460 --> 00:08:14,660
the structure of the Transformer we can

190
00:08:14,660 --> 00:08:18,000
divide into two macro blocks the first

191
00:08:18,000 --> 00:08:20,940
macro block is called encoder and it's

192
00:08:20,940 --> 00:08:22,500
this part here

193
00:08:22,500 --> 00:08:24,599
the second macro block is called a

194
00:08:24,599 --> 00:08:27,840
decoder and it's the second part here

195
00:08:27,840 --> 00:08:30,479
the third part here you see on the top

196
00:08:30,479 --> 00:08:32,399
it's just a linear layer and we will see

197
00:08:32,399 --> 00:08:35,700
why it's there and what it is function

198
00:08:35,700 --> 00:08:39,479
so and the two layers so the encoder and

199
00:08:39,479 --> 00:08:41,339
the decoder are connected by this

200
00:08:41,339 --> 00:08:43,620
connection you can see here

201
00:08:43,620 --> 00:08:46,440
in which some output of the encoder is

202
00:08:46,440 --> 00:08:48,899
sent as input to the decoder and we will

203
00:08:48,899 --> 00:08:51,959
also see how let's start first of all

204
00:08:51,959 --> 00:08:55,080
with some notations that I will be using

205
00:08:55,080 --> 00:08:58,620
during my explanation and you should be

206
00:08:58,620 --> 00:09:01,200
familiar with this notation also to

207
00:09:01,200 --> 00:09:03,839
review some maths so the first thing we

208
00:09:03,839 --> 00:09:05,880
should be familiar with is matrix

209
00:09:05,880 --> 00:09:08,040
multiplication so imagine we have a

210
00:09:08,040 --> 00:09:09,360
input Matrix

211
00:09:09,360 --> 00:09:12,680
which is a sequence of let's say words

212
00:09:12,680 --> 00:09:16,200
so sequence by D model and we will see

213
00:09:16,200 --> 00:09:17,820
why it's called sequence by the model so

214
00:09:17,820 --> 00:09:21,680
imagine we have a matrix that is a 6 by

215
00:09:21,680 --> 00:09:27,000
512 in which each row is a word

216
00:09:27,000 --> 00:09:29,279
and this word is not made of characters

217
00:09:29,279 --> 00:09:32,700
but by 512 numbers so each word is

218
00:09:32,700 --> 00:09:34,160
represented by

219
00:09:34,160 --> 00:09:37,680
512 numbers okay like this imagine you

220
00:09:37,680 --> 00:09:41,160
have 512 of them along this row 512

221
00:09:41,160 --> 00:09:44,279
along this other row etc etc one two

222
00:09:44,279 --> 00:09:46,080
three four five so we need another one

223
00:09:46,080 --> 00:09:49,320
here okay the first word we will call it

224
00:09:49,320 --> 00:09:54,839
a the second B the C D E and F

225
00:09:54,839 --> 00:09:57,600
if we multiply this matrix by another

226
00:09:57,600 --> 00:10:00,899
Matrix let's say the transpose of this

227
00:10:00,899 --> 00:10:04,019
Matrix so it's a matrix where the rows

228
00:10:04,019 --> 00:10:05,540
becomes columns

229
00:10:05,540 --> 00:10:08,640
so three

230
00:10:08,640 --> 00:10:11,240
four

231
00:10:12,000 --> 00:10:13,740
five and

232
00:10:13,740 --> 00:10:15,600
six

233
00:10:15,600 --> 00:10:22,320
this word will be here B C D E and F and

234
00:10:22,320 --> 00:10:24,980
then we have

235
00:10:25,260 --> 00:10:27,080
um

236
00:10:27,080 --> 00:10:31,620
512 numbers along each column because

237
00:10:31,620 --> 00:10:34,019
before we had them on the rows now they

238
00:10:34,019 --> 00:10:35,880
will become on the column so here we

239
00:10:35,880 --> 00:10:38,399
have the 512 number

240
00:10:38,399 --> 00:10:39,839
etc etc

241
00:10:39,839 --> 00:10:42,500
this is a matrix that is

242
00:10:42,500 --> 00:10:46,620
512 by 6 so let me add some brackets

243
00:10:46,620 --> 00:10:49,500
here if we multiply them we will get a

244
00:10:49,500 --> 00:10:52,860
new Matrix that is we cancel the inner

245
00:10:52,860 --> 00:10:55,260
dimensions and we get the outer

246
00:10:55,260 --> 00:10:58,079
Dimension so it will become six by six

247
00:10:58,079 --> 00:11:01,200
so it will be 6 rows by 6 rows so let's

248
00:11:01,200 --> 00:11:02,519
draw it

249
00:11:02,519 --> 00:11:04,620
how do we calculate the values of this

250
00:11:04,620 --> 00:11:08,339
output Matrix this is six by six

251
00:11:08,339 --> 00:11:12,480
this is the dot product of the first row

252
00:11:12,480 --> 00:11:14,880
with the First Column so this is a

253
00:11:14,880 --> 00:11:18,540
multiplied by a this the second value is

254
00:11:18,540 --> 00:11:21,540
the first row with the second column the

255
00:11:21,540 --> 00:11:24,540
third value is the first row with the

256
00:11:24,540 --> 00:11:25,800
third column

257
00:11:25,800 --> 00:11:29,820
until the last column so a multiplied by

258
00:11:29,820 --> 00:11:33,060
F Etc what is the dot product is

259
00:11:33,060 --> 00:11:35,820
basically you take the first number of

260
00:11:35,820 --> 00:11:38,940
the first row so here we have 512

261
00:11:38,940 --> 00:11:42,000
numbers here we have 512 numbers so you

262
00:11:42,000 --> 00:11:44,700
take the first number of the first row

263
00:11:44,700 --> 00:11:46,500
and the first number of the First Column

264
00:11:46,500 --> 00:11:48,120
you multiply them together

265
00:11:48,120 --> 00:11:51,240
second value of the first row second

266
00:11:51,240 --> 00:11:53,519
value of the First Column you multiply

267
00:11:53,519 --> 00:11:56,880
them together and then you add all these

268
00:11:56,880 --> 00:12:00,079
numbers together so it will be let's say

269
00:12:00,079 --> 00:12:03,300
uh this number multiplied by this plus

270
00:12:03,300 --> 00:12:06,180
this number multiplied by this plus this

271
00:12:06,180 --> 00:12:08,519
number multiplied by this plus this

272
00:12:08,519 --> 00:12:10,620
number multiplied by this plus you sum

273
00:12:10,620 --> 00:12:13,440
all this number together and this is the

274
00:12:13,440 --> 00:12:16,620
a DOT product a so we should be familiar

275
00:12:16,620 --> 00:12:18,180
with this notation because I will be

276
00:12:18,180 --> 00:12:21,060
using it a lot in the next slides let's

277
00:12:21,060 --> 00:12:23,220
start our journey with of the

278
00:12:23,220 --> 00:12:26,640
Transformer uh by looking at the encoder

279
00:12:26,640 --> 00:12:28,519
so the encoder

280
00:12:28,519 --> 00:12:31,560
starts with the input embeddings so what

281
00:12:31,560 --> 00:12:33,480
is an input embedding

282
00:12:33,480 --> 00:12:35,399
first of all let's start with our

283
00:12:35,399 --> 00:12:39,120
sentence we have a sentence of in this

284
00:12:39,120 --> 00:12:41,880
case six words what we do is we tokenize

285
00:12:41,880 --> 00:12:44,399
it we transform the sentence into tokens

286
00:12:44,399 --> 00:12:46,860
what does it mean to tokenize we split

287
00:12:46,860 --> 00:12:48,959
them into single words

288
00:12:48,959 --> 00:12:52,380
it is not necessary to always split the

289
00:12:52,380 --> 00:12:55,079
sentence using single words we can even

290
00:12:55,079 --> 00:12:58,019
split the sentence in part in smaller

291
00:12:58,019 --> 00:13:00,120
parts that are even smaller than a

292
00:13:00,120 --> 00:13:02,399
single word so we could even split this

293
00:13:02,399 --> 00:13:05,519
a sentence into let's say 20 tokens by

294
00:13:05,519 --> 00:13:08,519
using the each by splitting each word

295
00:13:08,519 --> 00:13:11,880
into multiple words this is usually done

296
00:13:11,880 --> 00:13:14,899
in most modern

297
00:13:14,899 --> 00:13:17,519
Transformer models but we will not be

298
00:13:17,519 --> 00:13:19,139
doing it otherwise it's really difficult

299
00:13:19,139 --> 00:13:21,899
to visualize so let's suppose we have

300
00:13:21,899 --> 00:13:24,899
this input sentence and we split into

301
00:13:24,899 --> 00:13:27,660
tokens and each token is a single word

302
00:13:27,660 --> 00:13:31,019
the next step we do is we map these

303
00:13:31,019 --> 00:13:33,120
words into numbers

304
00:13:33,120 --> 00:13:35,639
and these numbers represent the position

305
00:13:35,639 --> 00:13:38,579
of these words in our vocabulary so

306
00:13:38,579 --> 00:13:41,100
imagine we have a vocabulary of all the

307
00:13:41,100 --> 00:13:42,959
possible words that appear in our

308
00:13:42,959 --> 00:13:46,139
training set each word will occupy a

309
00:13:46,139 --> 00:13:48,180
position in this vocabulary so for

310
00:13:48,180 --> 00:13:50,040
example the word will occupy the

311
00:13:50,040 --> 00:13:52,560
position 105 the word the cat will

312
00:13:52,560 --> 00:13:55,500
occupy the position 6500

313
00:13:55,500 --> 00:13:56,519
Etc

314
00:13:56,519 --> 00:13:59,220
and as you can see this cat here has the

315
00:13:59,220 --> 00:14:01,139
same number as this cat here because

316
00:14:01,139 --> 00:14:02,700
they occupy the same position in the

317
00:14:02,700 --> 00:14:03,959
vocabulary

318
00:14:03,959 --> 00:14:06,240
we take these numbers which are called

319
00:14:06,240 --> 00:14:09,779
input IDs and we map them into a vector

320
00:14:09,779 --> 00:14:12,540
of size 512.

321
00:14:12,540 --> 00:14:15,839
this Vector is a vector made of 512

322
00:14:15,839 --> 00:14:16,980
numbers

323
00:14:16,980 --> 00:14:19,740
and we always map the same word to

324
00:14:19,740 --> 00:14:22,200
always the same embedding

325
00:14:22,200 --> 00:14:26,940
however this number is not fixed it's a

326
00:14:26,940 --> 00:14:29,100
parameter for our model so our model

327
00:14:29,100 --> 00:14:31,980
will learn to change these numbers in

328
00:14:31,980 --> 00:14:34,139
such a way that it represents the

329
00:14:34,139 --> 00:14:36,839
meaning of the word so the input ID is

330
00:14:36,839 --> 00:14:38,700
never change because our vocabulary is

331
00:14:38,700 --> 00:14:40,740
fixed but the embedding will change

332
00:14:40,740 --> 00:14:42,660
along with the training process of the

333
00:14:42,660 --> 00:14:45,600
model so the embeddings numbers will

334
00:14:45,600 --> 00:14:47,820
change according to the needs of the

335
00:14:47,820 --> 00:14:49,980
loss function so the input embedding are

336
00:14:49,980 --> 00:14:52,620
basically mapping our single word into

337
00:14:52,620 --> 00:14:55,860
an embedding of size 512 and we call

338
00:14:55,860 --> 00:14:59,220
this quantity 512 D model because it's

339
00:14:59,220 --> 00:15:01,199
the same name that it's also used in the

340
00:15:01,199 --> 00:15:04,940
paper attention is all you need

341
00:15:04,980 --> 00:15:07,019
let's look at the next layer of the

342
00:15:07,019 --> 00:15:10,560
encoder which is the positional encoding

343
00:15:10,560 --> 00:15:13,380
so what is positional encoding

344
00:15:13,380 --> 00:15:16,680
what we want is that each word should

345
00:15:16,680 --> 00:15:19,500
carry some information about its

346
00:15:19,500 --> 00:15:22,019
position in the sentence because now we

347
00:15:22,019 --> 00:15:25,019
built a matrix of words that are

348
00:15:25,019 --> 00:15:27,480
embeddings but they don't convey any

349
00:15:27,480 --> 00:15:30,600
information about how where that

350
00:15:30,600 --> 00:15:32,820
particular word is inside the sentence

351
00:15:32,820 --> 00:15:34,620
and this is the job of the positional

352
00:15:34,620 --> 00:15:37,139
encoding so what we do

353
00:15:37,139 --> 00:15:40,560
we want the model to treat words that

354
00:15:40,560 --> 00:15:42,540
appear close to each other as close and

355
00:15:42,540 --> 00:15:45,300
words that are distant as distant so we

356
00:15:45,300 --> 00:15:47,459
want the model to see this information

357
00:15:47,459 --> 00:15:49,740
about the special information that we

358
00:15:49,740 --> 00:15:51,779
see with our eyes so for example when we

359
00:15:51,779 --> 00:15:53,760
see this sentence what is positional

360
00:15:53,760 --> 00:15:56,820
encoding we know that the word what is

361
00:15:56,820 --> 00:15:59,699
more far from the word

362
00:15:59,699 --> 00:16:03,300
um is compared to encoding because we we

363
00:16:03,300 --> 00:16:05,040
have this partial information given by

364
00:16:05,040 --> 00:16:07,380
our eyes but the model cannot see this

365
00:16:07,380 --> 00:16:09,420
so we need to give some information to

366
00:16:09,420 --> 00:16:12,360
the model about how the words are

367
00:16:12,360 --> 00:16:14,220
specially distributed inside of the

368
00:16:14,220 --> 00:16:15,600
sentence

369
00:16:15,600 --> 00:16:18,360
and we want the positional encoding to

370
00:16:18,360 --> 00:16:20,940
represent a pattern that the model can

371
00:16:20,940 --> 00:16:24,360
learn and we will see how

372
00:16:24,360 --> 00:16:27,060
imagine we have our original sentence

373
00:16:27,060 --> 00:16:29,820
your cat is a lovely cat what we do is

374
00:16:29,820 --> 00:16:33,060
we first convert into embeddings using

375
00:16:33,060 --> 00:16:34,440
the previous layer so the input

376
00:16:34,440 --> 00:16:36,779
embeddings and these are embeddings of

377
00:16:36,779 --> 00:16:40,139
size 512 then we create some special

378
00:16:40,139 --> 00:16:42,060
vectors called the positional encoding

379
00:16:42,060 --> 00:16:45,180
vectors that we add to these embeddings

380
00:16:45,180 --> 00:16:48,600
so this Vector we see here in red

381
00:16:48,600 --> 00:16:52,740
is a vector of size 512 which is not

382
00:16:52,740 --> 00:16:54,959
learned it's computed once and not

383
00:16:54,959 --> 00:16:57,180
learned along with the training process

384
00:16:57,180 --> 00:17:00,540
it's fixed and this word this Vector

385
00:17:00,540 --> 00:17:02,399
represents the position of the word

386
00:17:02,399 --> 00:17:04,260
inside of the sentence

387
00:17:04,260 --> 00:17:08,459
and this should give us a output that is

388
00:17:08,459 --> 00:17:12,359
a vector of size again 512 because we

389
00:17:12,359 --> 00:17:15,720
are summing this number with this number

390
00:17:15,720 --> 00:17:18,179
this number with this number so the

391
00:17:18,179 --> 00:17:19,740
First Dimension with the First Dimension

392
00:17:19,740 --> 00:17:21,540
the second dimension with that so we

393
00:17:21,540 --> 00:17:23,819
will get a new Vector of the same size

394
00:17:23,819 --> 00:17:27,240
of the input vectors or how are these

395
00:17:27,240 --> 00:17:28,980
position in both embedding calculated

396
00:17:28,980 --> 00:17:30,200
let's see

397
00:17:30,200 --> 00:17:32,940
imagine we have a smaller sentence let's

398
00:17:32,940 --> 00:17:36,240
say your cat is and you may have seen

399
00:17:36,240 --> 00:17:39,240
the following expressions from the paper

400
00:17:39,240 --> 00:17:43,679
what we do is we create a vector of five

401
00:17:43,679 --> 00:17:48,360
of size D model so 512 and for each

402
00:17:48,360 --> 00:17:51,179
position in this Vector we calculate the

403
00:17:51,179 --> 00:17:54,240
value using these two expressions

404
00:17:54,240 --> 00:17:56,640
using these arguments so the first

405
00:17:56,640 --> 00:17:59,400
argument indicates the position of the

406
00:17:59,400 --> 00:18:01,919
word inside of the sentence so the word

407
00:18:01,919 --> 00:18:06,179
your occupies the position zero and we

408
00:18:06,179 --> 00:18:10,559
use them for the even Dimension so the

409
00:18:10,559 --> 00:18:14,100
zero the two the four the 510 Etc we use

410
00:18:14,100 --> 00:18:17,340
the first expression so the sine and for

411
00:18:17,340 --> 00:18:19,980
the other positions of this Vector we

412
00:18:19,980 --> 00:18:22,320
use the second expression

413
00:18:22,320 --> 00:18:24,720
and we do this for all the words inside

414
00:18:24,720 --> 00:18:27,179
of the sentence so this particular

415
00:18:27,179 --> 00:18:30,000
embedding is calculated p e of 1 0

416
00:18:30,000 --> 00:18:33,179
because it's the first word embedding

417
00:18:33,179 --> 00:18:36,780
zero so this one represents the argument

418
00:18:36,780 --> 00:18:40,440
pause and this 0 represents the argument

419
00:18:40,440 --> 00:18:46,559
2 I and p e of 1 1 means that the first

420
00:18:46,559 --> 00:18:50,039
word uh Dimension one so we will use the

421
00:18:50,039 --> 00:18:51,120
cosine

422
00:18:51,120 --> 00:18:53,760
giving the position one and the two I

423
00:18:53,760 --> 00:18:56,640
will be equal to 2i plus 1 will be equal

424
00:18:56,640 --> 00:18:58,559
to 1.

425
00:18:58,559 --> 00:19:03,120
and we do this for this third word Etc

426
00:19:03,120 --> 00:19:06,120
if we have another sentence we will not

427
00:19:06,120 --> 00:19:08,640
have different positional encodings

428
00:19:08,640 --> 00:19:12,000
we will have the same vectors even for

429
00:19:12,000 --> 00:19:14,400
different sentences because the

430
00:19:14,400 --> 00:19:16,620
positional encoding are computed once

431
00:19:16,620 --> 00:19:19,080
and reused for every sentence that our

432
00:19:19,080 --> 00:19:20,520
model will see

433
00:19:20,520 --> 00:19:23,820
during inference or training so we only

434
00:19:23,820 --> 00:19:25,919
compute the positional encoding once

435
00:19:25,919 --> 00:19:28,080
when we create the model we save them

436
00:19:28,080 --> 00:19:29,760
and then we reuse them we don't need to

437
00:19:29,760 --> 00:19:32,880
compute it every time we feed the feed a

438
00:19:32,880 --> 00:19:36,080
sentence to the model

439
00:19:36,360 --> 00:19:39,360
so why the authors chose the cosine and

440
00:19:39,360 --> 00:19:41,220
the sine functions to represent

441
00:19:41,220 --> 00:19:43,500
positional encodings because let's watch

442
00:19:43,500 --> 00:19:46,620
the plot of these two functions uh the

443
00:19:46,620 --> 00:19:49,140
you can see the plot is by position so

444
00:19:49,140 --> 00:19:50,880
the position of the word inside of the

445
00:19:50,880 --> 00:19:53,640
sentence and this depth is the dimension

446
00:19:53,640 --> 00:19:56,400
along the vector so the two I that you

447
00:19:56,400 --> 00:19:57,660
see saw before in the previous

448
00:19:57,660 --> 00:19:59,220
expressions

449
00:19:59,220 --> 00:20:01,860
and if we plot them we can see as humans

450
00:20:01,860 --> 00:20:04,200
a pattern here and we hope that the

451
00:20:04,200 --> 00:20:08,039
model can also see this path okay the

452
00:20:08,039 --> 00:20:09,720
next layer of the encoder is the

453
00:20:09,720 --> 00:20:11,780
multi-head attention

454
00:20:11,780 --> 00:20:16,320
we will not go inside of the multi-head

455
00:20:16,320 --> 00:20:18,539
attention first we will first visualize

456
00:20:18,539 --> 00:20:20,520
the single head attention so the

457
00:20:20,520 --> 00:20:23,220
self-attention with a single head and

458
00:20:23,220 --> 00:20:24,660
let's do it

459
00:20:24,660 --> 00:20:27,179
so what is self-attention self attention

460
00:20:27,179 --> 00:20:30,179
is a mechanism that existed before they

461
00:20:30,179 --> 00:20:33,000
introduced the Transformer the Alters of

462
00:20:33,000 --> 00:20:36,059
the Transformer just changed it into a

463
00:20:36,059 --> 00:20:38,580
multi-head attention so how did the

464
00:20:38,580 --> 00:20:40,980
self-attention work

465
00:20:40,980 --> 00:20:44,280
the self-attention allows the model to

466
00:20:44,280 --> 00:20:46,200
relate words to each other

467
00:20:46,200 --> 00:20:49,380
okay so we had the input embeddings that

468
00:20:49,380 --> 00:20:52,559
capture the meaning of the word then we

469
00:20:52,559 --> 00:20:54,960
have the positional encoding that give

470
00:20:54,960 --> 00:20:57,299
the information about the position of

471
00:20:57,299 --> 00:21:00,000
the word inside of the sentence now we

472
00:21:00,000 --> 00:21:02,580
want this self-attention to relate words

473
00:21:02,580 --> 00:21:04,020
to each other

474
00:21:04,020 --> 00:21:07,140
now imagine we have uh in an input

475
00:21:07,140 --> 00:21:09,780
sequence of six word with the D model of

476
00:21:09,780 --> 00:21:12,900
size 512.

477
00:21:12,900 --> 00:21:15,419
which can be represented as a matrix

478
00:21:15,419 --> 00:21:19,679
that we will call Q K and V so our q k

479
00:21:19,679 --> 00:21:22,919
and V is a same Matrix are the same

480
00:21:22,919 --> 00:21:25,679
Matrix representing the input so the

481
00:21:25,679 --> 00:21:29,159
input of six words with the dimension of

482
00:21:29,159 --> 00:21:32,039
512 so each word is represented by a

483
00:21:32,039 --> 00:21:36,059
vector of size 512. we basically apply

484
00:21:36,059 --> 00:21:38,220
this formula we saw here from the paper

485
00:21:38,220 --> 00:21:40,740
to calculate the attention the self

486
00:21:40,740 --> 00:21:41,880
attention in this case why

487
00:21:41,880 --> 00:21:44,700
self-attention because it's the each

488
00:21:44,700 --> 00:21:47,100
word in the sentence related to other

489
00:21:47,100 --> 00:21:49,320
words in the same sentence so it's

490
00:21:49,320 --> 00:21:51,720
self-attention

491
00:21:51,720 --> 00:21:54,659
so we start with our Q Matrix which is

492
00:21:54,659 --> 00:21:57,179
uh the input sentence so let's visualize

493
00:21:57,179 --> 00:21:59,820
it for example so we have six rows and

494
00:21:59,820 --> 00:22:03,120
on this uh on the columns we have 512

495
00:22:03,120 --> 00:22:04,740
column now they are really difficult to

496
00:22:04,740 --> 00:22:07,980
draw but let's say we have 512 columns

497
00:22:07,980 --> 00:22:12,539
and here we have six okay now what we do

498
00:22:12,539 --> 00:22:15,120
according to this formula we multiply it

499
00:22:15,120 --> 00:22:18,120
by the same sentence but transposed so

500
00:22:18,120 --> 00:22:20,340
the transpose of the K which is again

501
00:22:20,340 --> 00:22:22,799
the same input sequence

502
00:22:22,799 --> 00:22:26,280
we divide it by the square root of 512

503
00:22:26,280 --> 00:22:28,860
and then we apply this soft Max

504
00:22:28,860 --> 00:22:32,100
the output of this as we saw before in

505
00:22:32,100 --> 00:22:35,640
in the initial Matrix and notations we

506
00:22:35,640 --> 00:22:40,020
saw that when we multiply 6 by 512 with

507
00:22:40,020 --> 00:22:43,320
another Matrix that is 512 by 6 we

508
00:22:43,320 --> 00:22:45,659
obtain a new Matrix that is six by six

509
00:22:45,659 --> 00:22:48,840
and each value in this Matrix represents

510
00:22:48,840 --> 00:22:51,600
the dot product of the first row with

511
00:22:51,600 --> 00:22:54,299
the First Column this represents the dot

512
00:22:54,299 --> 00:22:56,280
product of the first row with the second

513
00:22:56,280 --> 00:22:58,440
column Etc

514
00:22:58,440 --> 00:23:00,780
the values here are actually randomly

515
00:23:00,780 --> 00:23:02,520
generated so don't concentrate on the

516
00:23:02,520 --> 00:23:04,500
values what you should notice is that

517
00:23:04,500 --> 00:23:07,740
the soft Max makes all these values in

518
00:23:07,740 --> 00:23:10,380
such a way that they sum up to one so

519
00:23:10,380 --> 00:23:13,559
this Row for example here some sums up

520
00:23:13,559 --> 00:23:16,620
to one this other row also sums up to

521
00:23:16,620 --> 00:23:20,880
one etc etc and this value we see here

522
00:23:20,880 --> 00:23:23,340
it's the dot product of the first word

523
00:23:23,340 --> 00:23:26,360
with the embedding of the word itself

524
00:23:26,360 --> 00:23:29,940
this value here is the dot product of

525
00:23:29,940 --> 00:23:33,120
the embedding of the word your with the

526
00:23:33,120 --> 00:23:36,900
embedding of the word cat and this value

527
00:23:36,900 --> 00:23:39,360
here is the dot product of the word the

528
00:23:39,360 --> 00:23:41,039
embedding of the word your with the

529
00:23:41,039 --> 00:23:43,380
embedding of the word is

530
00:23:43,380 --> 00:23:45,120
the next thing we and this value

531
00:23:45,120 --> 00:23:48,960
represents somehow a score that how

532
00:23:48,960 --> 00:23:51,539
intense is the relationship between one

533
00:23:51,539 --> 00:23:54,600
word and another let's go uh ahead with

534
00:23:54,600 --> 00:23:56,159
the formula so for now we just

535
00:23:56,159 --> 00:23:59,220
multiplied Q by K divided by the square

536
00:23:59,220 --> 00:24:01,799
root of Decay applied to the soft Max

537
00:24:01,799 --> 00:24:04,500
but we didn't multiply by V

538
00:24:04,500 --> 00:24:07,620
so let's go forward we multiply this

539
00:24:07,620 --> 00:24:10,500
matrix by V and we obtain a new Matrix

540
00:24:10,500 --> 00:24:13,860
which is 6 by 512 so if we multiply a

541
00:24:13,860 --> 00:24:16,080
matrix that is 6 by 6 with another that

542
00:24:16,080 --> 00:24:19,860
is 6 by 512 we get a new Matrix that is

543
00:24:19,860 --> 00:24:22,980
6 by 512 and one thing you should notice

544
00:24:22,980 --> 00:24:24,600
is that with the dimension of this

545
00:24:24,600 --> 00:24:26,940
Matrix is exactly the dimension of the

546
00:24:26,940 --> 00:24:29,780
initial Matrix from which we started

547
00:24:29,780 --> 00:24:33,419
this what does it mean that we obtain a

548
00:24:33,419 --> 00:24:35,940
new Matrix that is six rows so let's say

549
00:24:35,940 --> 00:24:37,440
six rows

550
00:24:37,440 --> 00:24:40,500
with 512 columns

551
00:24:40,500 --> 00:24:44,039
in which each these are our words so we

552
00:24:44,039 --> 00:24:46,020
have six words and each word has an

553
00:24:46,020 --> 00:24:49,799
embedding of Dimension 512 so now this

554
00:24:49,799 --> 00:24:53,039
embedding here represents not only the

555
00:24:53,039 --> 00:24:55,440
meaning of the word which was given by

556
00:24:55,440 --> 00:24:57,600
the input embedding not only the

557
00:24:57,600 --> 00:25:00,059
position of the word which was added by

558
00:25:00,059 --> 00:25:02,400
the positional encoding but now somehow

559
00:25:02,400 --> 00:25:04,980
this special embedding so these values

560
00:25:04,980 --> 00:25:07,679
represent a special embedding that also

561
00:25:07,679 --> 00:25:11,100
captures the relationship of this

562
00:25:11,100 --> 00:25:13,919
particular word with all the other words

563
00:25:13,919 --> 00:25:16,740
and this particular embedding of this

564
00:25:16,740 --> 00:25:19,740
word here also captures not only its

565
00:25:19,740 --> 00:25:22,260
meaning not only its position inside of

566
00:25:22,260 --> 00:25:24,240
the sentence but also the relationship

567
00:25:24,240 --> 00:25:27,240
of this word with all the other words

568
00:25:27,240 --> 00:25:29,220
I want to remind you that this is not

569
00:25:29,220 --> 00:25:31,020
the multi-head attention we are just

570
00:25:31,020 --> 00:25:33,480
watching the self-attention so one head

571
00:25:33,480 --> 00:25:36,539
we will we will see later how this

572
00:25:36,539 --> 00:25:40,100
becomes the multi-head attention

573
00:25:41,039 --> 00:25:43,320
self-attention has some properties that

574
00:25:43,320 --> 00:25:44,820
are very desirable

575
00:25:44,820 --> 00:25:47,640
first of all it's permutation invariant

576
00:25:47,640 --> 00:25:49,320
what does it mean to be permutation

577
00:25:49,320 --> 00:25:51,299
invariant it means that if we have a

578
00:25:51,299 --> 00:25:54,620
matrix let's say

579
00:25:54,740 --> 00:25:57,480
first we had a matrix of six words in

580
00:25:57,480 --> 00:25:59,159
this case the let's say just four words

581
00:25:59,159 --> 00:26:02,580
so a b c and d

582
00:26:02,580 --> 00:26:04,620
and suppose by applying the formula

583
00:26:04,620 --> 00:26:06,960
before this produces this particular

584
00:26:06,960 --> 00:26:09,299
Matrix in which the there is new special

585
00:26:09,299 --> 00:26:11,340
embedding

586
00:26:11,340 --> 00:26:13,740
for the word a a new special embedding

587
00:26:13,740 --> 00:26:15,419
for the word b a new special bedding for

588
00:26:15,419 --> 00:26:17,220
the word c and d so let's call it a

589
00:26:17,220 --> 00:26:19,559
prime B Prime C Prime D Prime if we

590
00:26:19,559 --> 00:26:22,320
change the position of these two rows

591
00:26:22,320 --> 00:26:24,900
the values will not change the position

592
00:26:24,900 --> 00:26:27,120
of the output will change accordingly so

593
00:26:27,120 --> 00:26:29,820
the values of B Prime will not change it

594
00:26:29,820 --> 00:26:32,039
will just change in the the position and

595
00:26:32,039 --> 00:26:34,200
also the C will also change position but

596
00:26:34,200 --> 00:26:36,000
the values in each Vector will not

597
00:26:36,000 --> 00:26:37,380
change and this is a desirable

598
00:26:37,380 --> 00:26:40,440
properties self-attention as of now

599
00:26:40,440 --> 00:26:42,960
requires no parameters I mean I didn't

600
00:26:42,960 --> 00:26:45,179
introduce any parameter that is learned

601
00:26:45,179 --> 00:26:47,760
by the model I just took the initial

602
00:26:47,760 --> 00:26:51,360
sentence of in this case six words

603
00:26:51,360 --> 00:26:54,480
we multiplied it by itself we divide it

604
00:26:54,480 --> 00:26:56,400
by a fixed quantity which is the square

605
00:26:56,400 --> 00:26:59,700
root of 512 and then we apply the soft

606
00:26:59,700 --> 00:27:01,980
Max which is not introducing any

607
00:27:01,980 --> 00:27:04,200
parameters so for now the self-attention

608
00:27:04,200 --> 00:27:06,539
rate didn't require any parameter except

609
00:27:06,539 --> 00:27:09,840
for the embedding of the words

610
00:27:09,840 --> 00:27:11,760
this will change later when we introduce

611
00:27:11,760 --> 00:27:14,400
the multi-head attention

612
00:27:14,400 --> 00:27:18,480
also we expect because the each value in

613
00:27:18,480 --> 00:27:20,279
the self-attention in the soft Max

614
00:27:20,279 --> 00:27:23,580
Matrix is a DOT product of the word

615
00:27:23,580 --> 00:27:25,620
embedding with itself and the other

616
00:27:25,620 --> 00:27:27,900
words we expect the values along the

617
00:27:27,900 --> 00:27:30,360
diagonal to be the maximum because it's

618
00:27:30,360 --> 00:27:33,059
the dot product dot product of each word

619
00:27:33,059 --> 00:27:34,860
with itself

620
00:27:34,860 --> 00:27:36,360
and

621
00:27:36,360 --> 00:27:39,299
there is another property of this Matrix

622
00:27:39,299 --> 00:27:42,779
that is before we apply the soft softmax

623
00:27:42,779 --> 00:27:46,679
if we replace the value in this Matrix

624
00:27:46,679 --> 00:27:49,080
suppose we don't want the word your and

625
00:27:49,080 --> 00:27:51,659
Cat to interact with each other or we

626
00:27:51,659 --> 00:27:53,880
don't want the word let's say is and the

627
00:27:53,880 --> 00:27:56,100
lovely to interact with each other what

628
00:27:56,100 --> 00:27:58,380
we can do is before we apply the softmax

629
00:27:58,380 --> 00:28:00,720
we can replace this value with minus

630
00:28:00,720 --> 00:28:04,500
infinity and also this value with minus

631
00:28:04,500 --> 00:28:06,539
infinity

632
00:28:06,539 --> 00:28:08,760
and when we apply the soft Max the soft

633
00:28:08,760 --> 00:28:12,000
Max will replace minus infinity with 0.

634
00:28:12,000 --> 00:28:14,760
because as you remember the soft Max is

635
00:28:14,760 --> 00:28:17,279
e to the power of x if x is going to

636
00:28:17,279 --> 00:28:19,380
minus infinity e will be e to the power

637
00:28:19,380 --> 00:28:21,539
of minus infinity will become very very

638
00:28:21,539 --> 00:28:25,260
close to zero so basically zero

639
00:28:25,260 --> 00:28:27,779
this is a desirable property that we

640
00:28:27,779 --> 00:28:29,880
will use in the decoder of the

641
00:28:29,880 --> 00:28:31,980
Transformer now let's have a look at

642
00:28:31,980 --> 00:28:34,500
what is a multi-head attention so what

643
00:28:34,500 --> 00:28:37,559
we just saw was the self attention and

644
00:28:37,559 --> 00:28:39,179
we want to convert it into a

645
00:28:39,179 --> 00:28:41,279
multi-headed tension you may have seen

646
00:28:41,279 --> 00:28:43,320
these expressions from the paper but

647
00:28:43,320 --> 00:28:45,059
don't worry I will explain them one by

648
00:28:45,059 --> 00:28:46,980
one so let's go

649
00:28:46,980 --> 00:28:49,559
imagine we have our encoder so we are on

650
00:28:49,559 --> 00:28:52,320
the encoder side of of the Transformer

651
00:28:52,320 --> 00:28:55,500
and we have our input sentence which is

652
00:28:55,500 --> 00:29:00,480
let's say 6 by 512 so Six Word by 512 is

653
00:29:00,480 --> 00:29:02,700
the size of the embedding of each word

654
00:29:02,700 --> 00:29:05,580
in this case I call it sequence by D

655
00:29:05,580 --> 00:29:07,799
model so sequence is the sequence length

656
00:29:07,799 --> 00:29:10,020
as you can see on the legend in the

657
00:29:10,020 --> 00:29:13,260
bottom left of the slide and the D model

658
00:29:13,260 --> 00:29:15,419
is the size of the embedding Vector

659
00:29:15,419 --> 00:29:19,140
which is 512. what we do just like the

660
00:29:19,140 --> 00:29:20,580
picture shows

661
00:29:20,580 --> 00:29:24,059
and we take this input and we make four

662
00:29:24,059 --> 00:29:27,659
copies of it one will be sent uh wait

663
00:29:27,659 --> 00:29:30,659
one will be sent along this connection

664
00:29:30,659 --> 00:29:33,659
we can see here and three will be sent

665
00:29:33,659 --> 00:29:36,600
to the multi-header attention with three

666
00:29:36,600 --> 00:29:39,059
respective names so it's the same input

667
00:29:39,059 --> 00:29:41,820
that becomes three matrices that are

668
00:29:41,820 --> 00:29:44,159
equal to input one is called the query

669
00:29:44,159 --> 00:29:46,020
one is called key and one is called

670
00:29:46,020 --> 00:29:48,720
value so basically we are taking this

671
00:29:48,720 --> 00:29:50,520
input and making three copies of it one

672
00:29:50,520 --> 00:29:53,460
we call Q K and B they have of course

673
00:29:53,460 --> 00:29:55,020
the same dimension

674
00:29:55,020 --> 00:29:57,000
what does the multihead attention do

675
00:29:57,000 --> 00:29:59,100
first of all it multiplies these three

676
00:29:59,100 --> 00:30:02,159
matrices by three parameter matrices

677
00:30:02,159 --> 00:30:06,380
called WQ w k and WV

678
00:30:06,380 --> 00:30:09,480
these matrices have Dimension D model by

679
00:30:09,480 --> 00:30:12,480
D model so if we multiply a matrix that

680
00:30:12,480 --> 00:30:14,100
is sequence by the model with another

681
00:30:14,100 --> 00:30:17,700
one that is D model by D model we get a

682
00:30:17,700 --> 00:30:20,640
new Matrix as output that is sequenced

683
00:30:20,640 --> 00:30:22,620
by D model so basically the same

684
00:30:22,620 --> 00:30:25,620
Dimension as the starting Matrix

685
00:30:25,620 --> 00:30:28,679
and we will call them Q Prime K Prime

686
00:30:28,679 --> 00:30:30,299
and V Prime

687
00:30:30,299 --> 00:30:33,720
our next step is to split these matrices

688
00:30:33,720 --> 00:30:36,840
into smaller matrices let's see how

689
00:30:36,840 --> 00:30:40,799
we can split this Matrix Q Prime by the

690
00:30:40,799 --> 00:30:43,020
sequence Dimension or by the D model

691
00:30:43,020 --> 00:30:44,460
dimension

692
00:30:44,460 --> 00:30:46,679
in the multi-hat attention we always

693
00:30:46,679 --> 00:30:50,039
split by the D model Dimension so every

694
00:30:50,039 --> 00:30:53,700
head will see the full sentence but a

695
00:30:53,700 --> 00:30:56,039
smaller part of the embedding of each

696
00:30:56,039 --> 00:30:57,059
word

697
00:30:57,059 --> 00:30:59,360
so if we have an embedding of let's say

698
00:30:59,360 --> 00:31:03,799
512 it will become smaller embeddings of

699
00:31:03,799 --> 00:31:07,620
512 divided by four and we call this

700
00:31:07,620 --> 00:31:11,100
quantity d k so d k is D model divided

701
00:31:11,100 --> 00:31:13,799
by H where H is the number of heads in

702
00:31:13,799 --> 00:31:17,340
our case we have H equal to 4.

703
00:31:17,340 --> 00:31:19,620
we can calculate the attention between

704
00:31:19,620 --> 00:31:23,220
these smaller matrices so q1 K1 and V1

705
00:31:23,220 --> 00:31:26,159
using the expression taken from the

706
00:31:26,159 --> 00:31:28,140
paper

707
00:31:28,140 --> 00:31:31,140
and this will result into a small Matrix

708
00:31:31,140 --> 00:31:35,279
called Head 1 head 2 head 3 and head

709
00:31:35,279 --> 00:31:39,120
four the dimension of head 1 up to head

710
00:31:39,120 --> 00:31:42,840
four is sequence by d v

711
00:31:42,840 --> 00:31:46,620
what is DV is basically it's equal to DK

712
00:31:46,620 --> 00:31:49,020
it's just called a DV because the last

713
00:31:49,020 --> 00:31:51,720
multiplication is done by V and in the

714
00:31:51,720 --> 00:31:53,580
paper they call it DV so I am also

715
00:31:53,580 --> 00:31:55,559
sticking to the same names

716
00:31:55,559 --> 00:32:00,419
our next step is to multi combine these

717
00:32:00,419 --> 00:32:03,179
matrices these small heads

718
00:32:03,179 --> 00:32:06,840
by concatenating them along the DV

719
00:32:06,840 --> 00:32:10,020
Dimension just like the paper says so we

720
00:32:10,020 --> 00:32:12,480
can cut all this head together and we

721
00:32:12,480 --> 00:32:16,380
get a new Matrix that is sequence by H

722
00:32:16,380 --> 00:32:18,480
multiplied by DV

723
00:32:18,480 --> 00:32:22,200
where H multiplied by DV as we know DV

724
00:32:22,200 --> 00:32:25,799
is equal to d k so H multiplied by DV is

725
00:32:25,799 --> 00:32:28,080
equal to D model so we get back the

726
00:32:28,080 --> 00:32:32,279
initial shape so it's sequence by D

727
00:32:32,279 --> 00:32:34,679
model here

728
00:32:34,679 --> 00:32:37,080
the next step is to multiply the result

729
00:32:37,080 --> 00:32:40,020
of this concatenation by w o

730
00:32:40,020 --> 00:32:43,799
and W O is a matrix that is H multiplied

731
00:32:43,799 --> 00:32:47,100
by DV so D model multiple with the other

732
00:32:47,100 --> 00:32:49,440
dimension being T model and the result

733
00:32:49,440 --> 00:32:52,140
of this is a new Matrix that is the

734
00:32:52,140 --> 00:32:54,179
result of the multi-head attention which

735
00:32:54,179 --> 00:32:56,700
is sequenced by D model

736
00:32:56,700 --> 00:33:00,000
so the multi had attention instead of

737
00:33:00,000 --> 00:33:02,940
calculating the attention between these

738
00:33:02,940 --> 00:33:06,779
matrices here so Q Prime K Prime and V

739
00:33:06,779 --> 00:33:09,779
Prime splits them along the D model

740
00:33:09,779 --> 00:33:12,419
Dimension into smaller matrices and

741
00:33:12,419 --> 00:33:14,519
calculates the attention between these

742
00:33:14,519 --> 00:33:17,399
smaller matrices so each head is

743
00:33:17,399 --> 00:33:19,919
watching the full sentence but as

744
00:33:19,919 --> 00:33:22,500
different aspect of the embedding of

745
00:33:22,500 --> 00:33:25,620
each word why we want this because we

746
00:33:25,620 --> 00:33:28,260
want the each head to watch different

747
00:33:28,260 --> 00:33:31,500
aspects of the same word for example in

748
00:33:31,500 --> 00:33:33,360
the Chinese language but also in other

749
00:33:33,360 --> 00:33:36,720
languages one word may be a noun in some

750
00:33:36,720 --> 00:33:38,940
cases maybe a verb in some other cases

751
00:33:38,940 --> 00:33:40,980
maybe a adverb in some other cases

752
00:33:40,980 --> 00:33:43,200
depending on the context

753
00:33:43,200 --> 00:33:46,760
so what we want is that one head maybe

754
00:33:46,760 --> 00:33:49,620
learns to relate that word as a noun

755
00:33:49,620 --> 00:33:52,260
another head maybe learns to relate that

756
00:33:52,260 --> 00:33:55,200
word as a verb and another head learn to

757
00:33:55,200 --> 00:33:57,480
release that verb as an objective or

758
00:33:57,480 --> 00:33:58,740
adverb

759
00:33:58,740 --> 00:34:01,620
so this is why we want a multi-head

760
00:34:01,620 --> 00:34:02,940
attention

761
00:34:02,940 --> 00:34:05,460
now you may also have seen online that

762
00:34:05,460 --> 00:34:09,960
the the attention can be visualized and

763
00:34:09,960 --> 00:34:12,839
I will show you how when we calculate

764
00:34:12,839 --> 00:34:15,560
the attention between the Q and the K

765
00:34:15,560 --> 00:34:19,199
matrices so when we do this operation so

766
00:34:19,199 --> 00:34:21,960
the soft Max of Q multiplied by the K

767
00:34:21,960 --> 00:34:24,780
divided by the square root of d k

768
00:34:24,780 --> 00:34:27,418
we get a new Matrix just like we saw

769
00:34:27,418 --> 00:34:29,699
before which is sequenced by sequence

770
00:34:29,699 --> 00:34:32,339
and this represents a score that

771
00:34:32,339 --> 00:34:34,320
represents the intensity of the

772
00:34:34,320 --> 00:34:36,960
relationship between the two words

773
00:34:36,960 --> 00:34:39,418
we can visualize this

774
00:34:39,418 --> 00:34:43,440
and this will produce a visualization uh

775
00:34:43,440 --> 00:34:45,060
similar to this one which I took from

776
00:34:45,060 --> 00:34:47,760
the paper in which we see how the all

777
00:34:47,760 --> 00:34:49,560
the heads work so for example if we

778
00:34:49,560 --> 00:34:52,320
concentrate on this work making this

779
00:34:52,320 --> 00:34:54,839
word here we can see that making is

780
00:34:54,839 --> 00:34:57,060
related to the word difficult so this

781
00:34:57,060 --> 00:35:00,060
word here by different heads so the blue

782
00:35:00,060 --> 00:35:03,140
head the red head and the green head

783
00:35:03,140 --> 00:35:07,380
but the wire let's say the Violet head

784
00:35:07,380 --> 00:35:09,420
is not relating this two word together

785
00:35:09,420 --> 00:35:11,520
so making and difficult is not related

786
00:35:11,520 --> 00:35:14,640
by the violet or the pink head

787
00:35:14,640 --> 00:35:17,400
The Violet head or the pink head they

788
00:35:17,400 --> 00:35:19,380
are relating the word making to other

789
00:35:19,380 --> 00:35:24,440
words for example to this word 2009

790
00:35:24,440 --> 00:35:27,420
why this is the case because maybe this

791
00:35:27,420 --> 00:35:30,000
pink head could see the part of the

792
00:35:30,000 --> 00:35:32,460
embedding that these other heads could

793
00:35:32,460 --> 00:35:35,040
not see that made this interaction

794
00:35:35,040 --> 00:35:38,780
possible between these two words

795
00:35:40,859 --> 00:35:43,500
you may be also wondering why these

796
00:35:43,500 --> 00:35:46,079
three mattresses are called query keys

797
00:35:46,079 --> 00:35:47,280
and values

798
00:35:47,280 --> 00:35:50,280
okay the terms come from the database

799
00:35:50,280 --> 00:35:52,440
terminology or from the python-like

800
00:35:52,440 --> 00:35:54,599
dictionaries but I would also like to

801
00:35:54,599 --> 00:35:56,880
give my interpretation of my own making

802
00:35:56,880 --> 00:35:58,859
a very simple example I think it's quite

803
00:35:58,859 --> 00:36:00,119
easy to

804
00:36:00,119 --> 00:36:00,839
um

805
00:36:00,839 --> 00:36:03,359
understand

806
00:36:03,359 --> 00:36:05,520
so imagine we have a python-like

807
00:36:05,520 --> 00:36:07,800
dictionary or a database in which we

808
00:36:07,800 --> 00:36:09,900
have keys and values

809
00:36:09,900 --> 00:36:13,200
the keys are the category of movies and

810
00:36:13,200 --> 00:36:15,599
the values are the movies belonging to

811
00:36:15,599 --> 00:36:18,300
that category in my case I just put one

812
00:36:18,300 --> 00:36:19,560
value

813
00:36:19,560 --> 00:36:22,740
so we have Romantics category which

814
00:36:22,740 --> 00:36:24,660
includes Titanic we have action movies

815
00:36:24,660 --> 00:36:27,240
that include the Dark Knight Etc imagine

816
00:36:27,240 --> 00:36:29,880
we also have a user that makes a query

817
00:36:29,880 --> 00:36:32,520
and the query is love

818
00:36:32,520 --> 00:36:34,560
because we are in the Transformer world

819
00:36:34,560 --> 00:36:37,020
all these words actually are represented

820
00:36:37,020 --> 00:36:40,260
by embeddings of size 512.

821
00:36:40,260 --> 00:36:43,260
so what our Transformer will do he will

822
00:36:43,260 --> 00:36:45,599
convert this word love into an embedding

823
00:36:45,599 --> 00:36:48,420
of 512 all these queries and values are

824
00:36:48,420 --> 00:36:52,079
already embeddings of 512 and it will

825
00:36:52,079 --> 00:36:54,540
calculate the dot product between the

826
00:36:54,540 --> 00:36:56,640
query and all the keys

827
00:36:56,640 --> 00:36:58,619
just like the formula so as you remember

828
00:36:58,619 --> 00:37:00,839
the formula is a soft Max of query

829
00:37:00,839 --> 00:37:03,119
multiplied by the transpose of the keys

830
00:37:03,119 --> 00:37:06,180
divided by the square root of the model

831
00:37:06,180 --> 00:37:08,640
so we are doing the dot product of all

832
00:37:08,640 --> 00:37:10,920
the queries with all the keys

833
00:37:10,920 --> 00:37:13,500
in this case the word love with all the

834
00:37:13,500 --> 00:37:15,060
keys one by one

835
00:37:15,060 --> 00:37:18,480
and this will result in a score that

836
00:37:18,480 --> 00:37:22,859
will amplify some values or not amplify

837
00:37:22,859 --> 00:37:25,859
other values

838
00:37:25,859 --> 00:37:28,980
um in this case our embedding may be in

839
00:37:28,980 --> 00:37:30,900
such a way that the word love and

840
00:37:30,900 --> 00:37:33,660
romantic are inter are related to each

841
00:37:33,660 --> 00:37:36,240
other the word love and comedy are also

842
00:37:36,240 --> 00:37:38,460
related to each other but not so

843
00:37:38,460 --> 00:37:40,800
intensively like the word love and

844
00:37:40,800 --> 00:37:44,160
romantic so it's more how to say let's

845
00:37:44,160 --> 00:37:46,680
less strong relationship but maybe the

846
00:37:46,680 --> 00:37:49,140
word horror and love are not related at

847
00:37:49,140 --> 00:37:51,420
all so maybe their soft Max score is

848
00:37:51,420 --> 00:37:54,560
very close to zero

849
00:37:56,220 --> 00:37:58,079
our next

850
00:37:58,079 --> 00:37:58,740
um

851
00:37:58,740 --> 00:38:01,760
layer in the encoder is the ADD and norm

852
00:38:01,760 --> 00:38:04,380
and to introduce the other Norm we need

853
00:38:04,380 --> 00:38:06,359
the layer normalization so let's see

854
00:38:06,359 --> 00:38:09,180
what is the layer normalization

855
00:38:09,180 --> 00:38:13,079
layer normalization is a layer that okay

856
00:38:13,079 --> 00:38:15,480
let's make a practical example imagine

857
00:38:15,480 --> 00:38:19,380
we have a batch of n items in this case

858
00:38:19,380 --> 00:38:22,320
n is equal to three

859
00:38:22,320 --> 00:38:25,260
item one item two item three each of

860
00:38:25,260 --> 00:38:28,320
these items will have some features it

861
00:38:28,320 --> 00:38:30,599
could be an embedding so for example it

862
00:38:30,599 --> 00:38:32,640
could be a feature of a vector of size

863
00:38:32,640 --> 00:38:35,760
512 but it could be a very big Matrix of

864
00:38:35,760 --> 00:38:38,099
thousands of features doesn't matter

865
00:38:38,099 --> 00:38:40,619
what we do is we calculate the mean and

866
00:38:40,619 --> 00:38:42,720
the variance of each of these items

867
00:38:42,720 --> 00:38:44,820
independently from each other

868
00:38:44,820 --> 00:38:47,400
and we replace each value with another

869
00:38:47,400 --> 00:38:49,500
value that is given by this expression

870
00:38:49,500 --> 00:38:51,960
so basically we are normalizing so that

871
00:38:51,960 --> 00:38:54,900
the new values are all in the range 0 to

872
00:38:54,900 --> 00:38:56,700
1.

873
00:38:56,700 --> 00:38:59,780
actually we also multiply this new value

874
00:38:59,780 --> 00:39:03,540
with a parameter called gamma and then

875
00:39:03,540 --> 00:39:06,540
we add another parameter called beta and

876
00:39:06,540 --> 00:39:08,579
this gamma and beta are learnable

877
00:39:08,579 --> 00:39:09,900
parameters

878
00:39:09,900 --> 00:39:13,200
and the model should learn to multiply

879
00:39:13,200 --> 00:39:16,500
and add these parameters so as to

880
00:39:16,500 --> 00:39:19,140
amplify the value that it wants to be

881
00:39:19,140 --> 00:39:21,420
Amplified and not amplify that value

882
00:39:21,420 --> 00:39:24,599
that it doesn't want to be Amplified

883
00:39:24,599 --> 00:39:27,420
uh so we don't just normalize we

884
00:39:27,420 --> 00:39:30,000
actually introduce some parameters

885
00:39:30,000 --> 00:39:32,880
and I found a really nice visualization

886
00:39:32,880 --> 00:39:35,160
from papers with code.com

887
00:39:35,160 --> 00:39:36,900
in which we see the difference between

888
00:39:36,900 --> 00:39:39,720
batch norm and layer Norm so as we can

889
00:39:39,720 --> 00:39:42,060
see in the layer normalization we are

890
00:39:42,060 --> 00:39:45,180
calculating if n is the batch Dimension

891
00:39:45,180 --> 00:39:47,640
we are calculating all the values

892
00:39:47,640 --> 00:39:50,820
belonging to one item in the batch

893
00:39:50,820 --> 00:39:53,400
while in the batch Norm we are

894
00:39:53,400 --> 00:39:56,820
calculating the same feature for all the

895
00:39:56,820 --> 00:39:59,579
batch so for all the items in the batch

896
00:39:59,579 --> 00:40:02,820
so we are mixing let's say values from

897
00:40:02,820 --> 00:40:05,280
different items of the batch while in

898
00:40:05,280 --> 00:40:07,140
the layer normalization we are treating

899
00:40:07,140 --> 00:40:09,300
each item in the batch independently

900
00:40:09,300 --> 00:40:11,520
which will have its own mean and its own

901
00:40:11,520 --> 00:40:14,119
variance

902
00:40:14,760 --> 00:40:18,140
let's look at the decoder now

903
00:40:18,240 --> 00:40:21,060
um in the encoder we saw the input

904
00:40:21,060 --> 00:40:23,040
embeddings in this call in this case

905
00:40:23,040 --> 00:40:24,780
they are called output embeddings but

906
00:40:24,780 --> 00:40:28,200
the underlying working is the same here

907
00:40:28,200 --> 00:40:30,720
also we have the positional encoding and

908
00:40:30,720 --> 00:40:34,500
they are also the same as the Imp as the

909
00:40:34,500 --> 00:40:35,640
encoder

910
00:40:35,640 --> 00:40:38,040
the next layer is the musket multi-head

911
00:40:38,040 --> 00:40:41,760
attention and we will see it now we also

912
00:40:41,760 --> 00:40:44,099
have the multi-head attention here with

913
00:40:44,099 --> 00:40:45,420
the

914
00:40:45,420 --> 00:40:48,540
here we should see that the

915
00:40:48,540 --> 00:40:51,420
there is the encoder here that produces

916
00:40:51,420 --> 00:40:55,200
the output and is sent to the decoder in

917
00:40:55,200 --> 00:40:59,220
the forms of keys

918
00:40:59,220 --> 00:41:01,800
and values

919
00:41:01,800 --> 00:41:04,619
while the query so this connection here

920
00:41:04,619 --> 00:41:07,920
is the query coming from the decoder

921
00:41:07,920 --> 00:41:11,339
so in this multi-head attention it's not

922
00:41:11,339 --> 00:41:13,680
a self-attention anymore it's a cross

923
00:41:13,680 --> 00:41:15,599
attention because we are taking two

924
00:41:15,599 --> 00:41:18,240
sentences one is sent from the encoder

925
00:41:18,240 --> 00:41:21,480
side so let's write encoder in which we

926
00:41:21,480 --> 00:41:23,760
provide the output of the encoder and we

927
00:41:23,760 --> 00:41:26,700
use it as a query as keys and values

928
00:41:26,700 --> 00:41:28,980
while the output of the masked

929
00:41:28,980 --> 00:41:31,380
multi-head attention is used as the

930
00:41:31,380 --> 00:41:34,380
query in this multi-head attention

931
00:41:34,380 --> 00:41:36,780
and the musket multi-head attention is

932
00:41:36,780 --> 00:41:40,079
the self-attention of the input sentence

933
00:41:40,079 --> 00:41:41,700
of the decoder so we take the input

934
00:41:41,700 --> 00:41:44,160
sentence of the decoder we transform

935
00:41:44,160 --> 00:41:47,099
into embeddings we add the depositional

936
00:41:47,099 --> 00:41:49,020
encoding we give it to this multi-head

937
00:41:49,020 --> 00:41:50,940
attention in which the query key and

938
00:41:50,940 --> 00:41:54,420
values are the same input sequence we do

939
00:41:54,420 --> 00:41:58,200
the ADD and Norm then we send this as

940
00:41:58,200 --> 00:42:00,720
the queries of the multi-head attention

941
00:42:00,720 --> 00:42:02,579
while the keys and the values are coming

942
00:42:02,579 --> 00:42:04,619
from the encoder then we do the add the

943
00:42:04,619 --> 00:42:05,339
norm

944
00:42:05,339 --> 00:42:07,619
I will not be showing the feed forward

945
00:42:07,619 --> 00:42:10,619
which is just a fully connected layer

946
00:42:10,619 --> 00:42:12,480
we then send the output of the feed

947
00:42:12,480 --> 00:42:15,660
forward to the ADD and norm and finally

948
00:42:15,660 --> 00:42:17,940
to the linear layer which we will see

949
00:42:17,940 --> 00:42:20,400
later so let's have a look at the Muscat

950
00:42:20,400 --> 00:42:22,619
multi-head attention and how it differs

951
00:42:22,619 --> 00:42:25,800
from a normal multi-head attention

952
00:42:25,800 --> 00:42:29,099
what we want our goal is that we want to

953
00:42:29,099 --> 00:42:32,280
make the model causal it means that the

954
00:42:32,280 --> 00:42:34,500
output at a certain position can only

955
00:42:34,500 --> 00:42:36,839
depend on the words on the previous

956
00:42:36,839 --> 00:42:39,660
position so the model must not be able

957
00:42:39,660 --> 00:42:42,480
to see future words how can we achieve

958
00:42:42,480 --> 00:42:43,859
that

959
00:42:43,859 --> 00:42:47,460
as you saw the the output of the soft

960
00:42:47,460 --> 00:42:49,800
Max in the attention calculation formula

961
00:42:49,800 --> 00:42:52,980
is this Matrix sequence by sequence if

962
00:42:52,980 --> 00:42:55,440
we want to hide the interaction of some

963
00:42:55,440 --> 00:42:58,079
words with other words we delete this

964
00:42:58,079 --> 00:43:00,660
value and we replace it with minus

965
00:43:00,660 --> 00:43:03,660
infinity before we apply the soft Max so

966
00:43:03,660 --> 00:43:06,780
that the soft Max will replace this

967
00:43:06,780 --> 00:43:10,319
value with 0. and we do this for all the

968
00:43:10,319 --> 00:43:12,720
interaction that we don't want so we

969
00:43:12,720 --> 00:43:15,300
don't want your to watch future words so

970
00:43:15,300 --> 00:43:18,060
we don't want your to watch cat is a

971
00:43:18,060 --> 00:43:20,099
lovely cat and we don't want the word

972
00:43:20,099 --> 00:43:22,560
cat to watch future words but only all

973
00:43:22,560 --> 00:43:24,960
the words that come before it or the

974
00:43:24,960 --> 00:43:27,599
word itself so we don't want this this

975
00:43:27,599 --> 00:43:30,420
this this also the same for the other

976
00:43:30,420 --> 00:43:32,640
words Etc

977
00:43:32,640 --> 00:43:36,000
so we can see that we are replacing all

978
00:43:36,000 --> 00:43:39,660
the word all this values here that are

979
00:43:39,660 --> 00:43:42,599
above this diagonal here so this is the

980
00:43:42,599 --> 00:43:44,880
principal diagonal of the Matrix and we

981
00:43:44,880 --> 00:43:47,099
want all the values that are above this

982
00:43:47,099 --> 00:43:49,859
diagonal to be replaced with minus

983
00:43:49,859 --> 00:43:53,040
infinity so that so that the soft Max

984
00:43:53,040 --> 00:43:55,920
will replace them with zero let's see in

985
00:43:55,920 --> 00:43:57,960
which stage of the multi-head attention

986
00:43:57,960 --> 00:44:01,680
this mechanism is introduced so when we

987
00:44:01,680 --> 00:44:05,220
calculate the attention between these

988
00:44:05,220 --> 00:44:08,839
smaller matrices so q1 K1 and V1

989
00:44:08,839 --> 00:44:12,000
before we apply this soft Max we replace

990
00:44:12,000 --> 00:44:15,480
this values so this one this one this

991
00:44:15,480 --> 00:44:18,119
one this one this one Etc with minus

992
00:44:18,119 --> 00:44:21,000
infinity then we apply this soft Max and

993
00:44:21,000 --> 00:44:23,359
then the soft Max will take care of

994
00:44:23,359 --> 00:44:26,460
transforming these values into zeros so

995
00:44:26,460 --> 00:44:28,980
basically we don't want these words to

996
00:44:28,980 --> 00:44:30,619
interact with each other

997
00:44:30,619 --> 00:44:33,119
and if we don't want this interaction

998
00:44:33,119 --> 00:44:35,160
the model will learn to not make them

999
00:44:35,160 --> 00:44:37,020
interact because the model will not get

1000
00:44:37,020 --> 00:44:39,359
any information from this interaction so

1001
00:44:39,359 --> 00:44:41,940
it's like this word cannot interact now

1002
00:44:41,940 --> 00:44:43,920
let's look at how the inference and

1003
00:44:43,920 --> 00:44:46,560
training works for a Transformer model

1004
00:44:46,560 --> 00:44:50,579
as I saw said previously we are dealing

1005
00:44:50,579 --> 00:44:52,140
with it we will be dealing with the

1006
00:44:52,140 --> 00:44:54,060
translation tasks so because it's easy

1007
00:44:54,060 --> 00:44:56,099
to visualize and it's easy to understand

1008
00:44:56,099 --> 00:44:59,520
all the steps let's start with the

1009
00:44:59,520 --> 00:45:02,339
training of the model we will go from an

1010
00:45:02,339 --> 00:45:04,079
English sentence I love you very much

1011
00:45:04,079 --> 00:45:07,079
into an Italian sentence it's a very

1012
00:45:07,079 --> 00:45:11,160
simple sentence it's easy to describe

1013
00:45:11,160 --> 00:45:12,599
let's go

1014
00:45:12,599 --> 00:45:16,500
we start with a description of the

1015
00:45:16,500 --> 00:45:19,020
of the Transformer model and we start

1016
00:45:19,020 --> 00:45:21,119
with our English sentence which is sent

1017
00:45:21,119 --> 00:45:24,060
to the encoder so our English sentence

1018
00:45:24,060 --> 00:45:25,020
here

1019
00:45:25,020 --> 00:45:28,140
on which we prepared and append to

1020
00:45:28,140 --> 00:45:30,720
special tokens one is called start of

1021
00:45:30,720 --> 00:45:32,640
sentence and one is called end of

1022
00:45:32,640 --> 00:45:36,240
sentence these two tokens are taken from

1023
00:45:36,240 --> 00:45:37,859
the vocabulary so they are special

1024
00:45:37,859 --> 00:45:40,980
tokens in our vocabulary that tells the

1025
00:45:40,980 --> 00:45:43,740
model what is the start position of a

1026
00:45:43,740 --> 00:45:45,720
sentence and what is the end of a

1027
00:45:45,720 --> 00:45:48,000
sentence we will see later why we need

1028
00:45:48,000 --> 00:45:49,079
them

1029
00:45:49,079 --> 00:45:50,940
for now just think that we take our

1030
00:45:50,940 --> 00:45:53,339
sentence we prepend a special token and

1031
00:45:53,339 --> 00:45:55,740
we append a special token

1032
00:45:55,740 --> 00:45:58,079
then what we do as you can see from the

1033
00:45:58,079 --> 00:46:00,720
picture we take our inputs we transform

1034
00:46:00,720 --> 00:46:02,700
into input embeddings we add the

1035
00:46:02,700 --> 00:46:04,500
positional encoding and then we send it

1036
00:46:04,500 --> 00:46:06,420
to the encoder

1037
00:46:06,420 --> 00:46:08,700
so this is our encoder input sequence by

1038
00:46:08,700 --> 00:46:11,040
the model we send it to the encoder it

1039
00:46:11,040 --> 00:46:14,040
will produce an output which is encode a

1040
00:46:14,040 --> 00:46:16,260
sequence by D model and it's called the

1041
00:46:16,260 --> 00:46:19,319
encoder output so as I saw we saw

1042
00:46:19,319 --> 00:46:21,960
previously the output of the encoder is

1043
00:46:21,960 --> 00:46:23,819
another Matrix that has the same

1044
00:46:23,819 --> 00:46:26,700
Dimension as the input Matrix

1045
00:46:26,700 --> 00:46:30,720
in which the embedding we can see it as

1046
00:46:30,720 --> 00:46:32,819
a sequence of embeddings in which this

1047
00:46:32,819 --> 00:46:34,920
embedding is special because it captures

1048
00:46:34,920 --> 00:46:37,079
not only the meaning of the word which

1049
00:46:37,079 --> 00:46:39,420
was given by the input embedding we saw

1050
00:46:39,420 --> 00:46:42,359
here so by this not only the position

1051
00:46:42,359 --> 00:46:44,220
which was given by the positional

1052
00:46:44,220 --> 00:46:47,099
encoding but also the interaction of

1053
00:46:47,099 --> 00:46:49,980
every word with every other word in the

1054
00:46:49,980 --> 00:46:51,720
same sentence because this is the

1055
00:46:51,720 --> 00:46:53,960
encoder so we are talking about

1056
00:46:53,960 --> 00:46:56,640
self-attention so it's the interaction

1057
00:46:56,640 --> 00:46:59,099
of each word in the sentence with all

1058
00:46:59,099 --> 00:47:02,819
the other words in the same sentence

1059
00:47:02,819 --> 00:47:04,920
we want to convert this sentence into

1060
00:47:04,920 --> 00:47:07,319
Italian so we prepare the input of the

1061
00:47:07,319 --> 00:47:11,940
decoder which is a start of sentence

1062
00:47:11,940 --> 00:47:14,160
as you can see from the picture of the

1063
00:47:14,160 --> 00:47:17,280
the Transformer the outputs here you can

1064
00:47:17,280 --> 00:47:18,780
see shifted right

1065
00:47:18,780 --> 00:47:20,040
what does it mean to shift right

1066
00:47:20,040 --> 00:47:21,960
basically it means we prepared a special

1067
00:47:21,960 --> 00:47:26,180
token called SOS start of sentence

1068
00:47:26,180 --> 00:47:30,200
you should also notice that these two

1069
00:47:30,200 --> 00:47:33,839
sequences actually they in when we code

1070
00:47:33,839 --> 00:47:36,240
the Transformer so if you watch my other

1071
00:47:36,240 --> 00:47:38,040
video on how to code a Transformer you

1072
00:47:38,040 --> 00:47:39,240
will see that we

1073
00:47:39,240 --> 00:47:42,119
make this sequence of fixed length so

1074
00:47:42,119 --> 00:47:43,980
that if we have a sentence that is te

1075
00:47:43,980 --> 00:47:45,780
amo multo or a very long sequence

1076
00:47:45,780 --> 00:47:48,480
actually when we feed them to the

1077
00:47:48,480 --> 00:47:50,819
Transformer they all becomes become of

1078
00:47:50,819 --> 00:47:54,599
the same length how to do this we add

1079
00:47:54,599 --> 00:47:57,660
padding words to reach the length the

1080
00:47:57,660 --> 00:47:59,940
desired length so if our model can

1081
00:47:59,940 --> 00:48:01,859
support let's say a sequence length of

1082
00:48:01,859 --> 00:48:04,859
1000 in this case we have a fourth

1083
00:48:04,859 --> 00:48:07,400
tokens we will add

1084
00:48:07,400 --> 00:48:10,680
996 tokens of padding to make this

1085
00:48:10,680 --> 00:48:12,660
sentence long enough to reach the

1086
00:48:12,660 --> 00:48:14,579
sequence length of course I'm not doing

1087
00:48:14,579 --> 00:48:16,380
it here because it's not easy to

1088
00:48:16,380 --> 00:48:18,300
visualize otherwise

1089
00:48:18,300 --> 00:48:20,520
okay we prepared this input for the

1090
00:48:20,520 --> 00:48:24,960
decoder we add transform into embeddings

1091
00:48:24,960 --> 00:48:27,660
we add the positional encoding then we

1092
00:48:27,660 --> 00:48:29,160
send it first to the multi-head

1093
00:48:29,160 --> 00:48:30,720
attentions to the musket

1094
00:48:30,720 --> 00:48:32,400
multi-haditation so along with the

1095
00:48:32,400 --> 00:48:33,900
causal mask

1096
00:48:33,900 --> 00:48:37,500
and then we take the output of the

1097
00:48:37,500 --> 00:48:40,339
encoder and we send it to the decoder as

1098
00:48:40,339 --> 00:48:44,700
keys and values while the queries are

1099
00:48:44,700 --> 00:48:46,920
coming from the musket so the queries

1100
00:48:46,920 --> 00:48:49,440
are coming from this layer and the keys

1101
00:48:49,440 --> 00:48:51,119
and the values are the output of the

1102
00:48:51,119 --> 00:48:52,319
encoder

1103
00:48:52,319 --> 00:48:55,680
this the output of all this block here

1104
00:48:55,680 --> 00:48:58,859
so all this big block here

1105
00:48:58,859 --> 00:49:01,560
will be a matrix that is sequence by the

1106
00:49:01,560 --> 00:49:04,859
model just like for the encoder

1107
00:49:04,859 --> 00:49:08,819
however we can see that this is still an

1108
00:49:08,819 --> 00:49:11,220
embedding because it's a D model it's a

1109
00:49:11,220 --> 00:49:14,099
vector of size 512 how can we relate

1110
00:49:14,099 --> 00:49:15,119
this

1111
00:49:15,119 --> 00:49:17,819
um embedding back into our dictionary

1112
00:49:17,819 --> 00:49:21,060
how can we understand what is this word

1113
00:49:21,060 --> 00:49:24,420
in our vocabulary that's why we need a

1114
00:49:24,420 --> 00:49:28,260
linear layer that will map sequence by D

1115
00:49:28,260 --> 00:49:30,359
model into another sequence by

1116
00:49:30,359 --> 00:49:33,000
vocabulary size so it will tell for

1117
00:49:33,000 --> 00:49:35,579
every embedding that it sees what is the

1118
00:49:35,579 --> 00:49:38,700
position of that word in our vocabulary

1119
00:49:38,700 --> 00:49:40,920
so that we can understand what is the

1120
00:49:40,920 --> 00:49:45,560
actual token that is output by the model

1121
00:49:46,079 --> 00:49:49,819
after that we apply the softmax and

1122
00:49:49,819 --> 00:49:52,920
then we have our label what we expect

1123
00:49:52,920 --> 00:49:56,700
the model to Output given this English

1124
00:49:56,700 --> 00:49:59,700
sentence

1125
00:49:59,700 --> 00:50:02,880
we expect the model to Output this te

1126
00:50:02,880 --> 00:50:06,359
amo multo end of sentence and this is

1127
00:50:06,359 --> 00:50:09,119
called the label or the target what we

1128
00:50:09,119 --> 00:50:11,220
do when we have the output of the model

1129
00:50:11,220 --> 00:50:13,800
and the corresponding label we calculate

1130
00:50:13,800 --> 00:50:15,839
the loss in this case is the cross

1131
00:50:15,839 --> 00:50:18,780
entropy loss and then we back propagate

1132
00:50:18,780 --> 00:50:21,119
the loss to all the weights

1133
00:50:21,119 --> 00:50:23,819
now let's understand why we have these

1134
00:50:23,819 --> 00:50:26,700
special tokens called SOS and EOS

1135
00:50:26,700 --> 00:50:28,859
basically you can see that here the

1136
00:50:28,859 --> 00:50:31,619
sequence length is 4 actually is 1000

1137
00:50:31,619 --> 00:50:33,480
because we have the padding but let's

1138
00:50:33,480 --> 00:50:35,099
say we don't have any padding so it's

1139
00:50:35,099 --> 00:50:37,560
four tokens start of sentence the ammo

1140
00:50:37,560 --> 00:50:40,980
multo and what we want is the T ammo

1141
00:50:40,980 --> 00:50:43,859
multo end of sentence so our model when

1142
00:50:43,859 --> 00:50:46,740
it will see the start of sentence token

1143
00:50:46,740 --> 00:50:50,760
it will output the first token as output

1144
00:50:50,760 --> 00:50:55,980
T when it will see T it will output ammo

1145
00:50:55,980 --> 00:50:58,800
when it will see armor it will output

1146
00:50:58,800 --> 00:51:02,339
molto and when it will see a multo it

1147
00:51:02,339 --> 00:51:04,680
will output end of sentence which will

1148
00:51:04,680 --> 00:51:06,480
indicate that okay the translation is

1149
00:51:06,480 --> 00:51:07,260
done

1150
00:51:07,260 --> 00:51:09,839
and we will see this mechanism in the

1151
00:51:09,839 --> 00:51:12,380
inference

1152
00:51:12,780 --> 00:51:16,020
ah this all happens in one time step

1153
00:51:16,020 --> 00:51:18,480
just like I promised at the beginning of

1154
00:51:18,480 --> 00:51:21,300
the video I said that with recurrental

1155
00:51:21,300 --> 00:51:23,760
or neural networks we have end time

1156
00:51:23,760 --> 00:51:28,079
steps to map n input sequence into an

1157
00:51:28,079 --> 00:51:30,900
output sequence but this problem would

1158
00:51:30,900 --> 00:51:33,900
be solved with the Transformer yes it

1159
00:51:33,900 --> 00:51:35,579
has been solved because you can see here

1160
00:51:35,579 --> 00:51:38,880
we didn't do any for Loop we just did

1161
00:51:38,880 --> 00:51:41,400
all in one pass we give an input

1162
00:51:41,400 --> 00:51:43,800
sequence to the encoder an input

1163
00:51:43,800 --> 00:51:46,740
sequence to the decoder we produced some

1164
00:51:46,740 --> 00:51:49,559
outputs we calculated that cross entropy

1165
00:51:49,559 --> 00:51:52,559
loss with the label and that's it it all

1166
00:51:52,559 --> 00:51:55,440
happens in one time step and this is the

1167
00:51:55,440 --> 00:51:57,420
power of the Transformer because it made

1168
00:51:57,420 --> 00:52:00,540
it very easy and very fast to train very

1169
00:52:00,540 --> 00:52:03,540
long sequences and with the very very

1170
00:52:03,540 --> 00:52:05,400
nice performance that you can see in

1171
00:52:05,400 --> 00:52:09,800
charge GPD you can see GPT in bird Etc

1172
00:52:10,140 --> 00:52:14,220
let's have a look at how inference works

1173
00:52:14,220 --> 00:52:16,500
again we have our English sentence I

1174
00:52:16,500 --> 00:52:18,420
love you very much we want to map it

1175
00:52:18,420 --> 00:52:21,800
into an Italian sentence

1176
00:52:22,500 --> 00:52:25,500
we have our usual Transformer we prepare

1177
00:52:25,500 --> 00:52:28,500
the input for the encoder which is start

1178
00:52:28,500 --> 00:52:30,300
of sentence I love you very much end of

1179
00:52:30,300 --> 00:52:31,740
sentence

1180
00:52:31,740 --> 00:52:34,260
we convert into input embeddings then we

1181
00:52:34,260 --> 00:52:35,760
add the positional encoding we prepare

1182
00:52:35,760 --> 00:52:37,500
the input for the encoder and we send it

1183
00:52:37,500 --> 00:52:40,079
to the encoder the encoder will produce

1184
00:52:40,079 --> 00:52:42,359
an output which is sequenced by the

1185
00:52:42,359 --> 00:52:44,640
model and we saw it before that it's a

1186
00:52:44,640 --> 00:52:46,559
sequence of special embeddings that

1187
00:52:46,559 --> 00:52:48,240
capture the meaning the position but

1188
00:52:48,240 --> 00:52:49,980
also the interaction of all the words

1189
00:52:49,980 --> 00:52:52,440
with other words

1190
00:52:52,440 --> 00:52:56,099
what we do is for the decoder we give

1191
00:52:56,099 --> 00:52:59,220
him just the start of sentence and of

1192
00:52:59,220 --> 00:53:01,140
course we keep the we add enough

1193
00:53:01,140 --> 00:53:03,480
embedding padding tokens to reach our

1194
00:53:03,480 --> 00:53:06,420
sequence length we just give the model

1195
00:53:06,420 --> 00:53:09,540
the start of sentence token and we again

1196
00:53:09,540 --> 00:53:12,740
we for this single token we convert into

1197
00:53:12,740 --> 00:53:14,819
embeddings we add the positional

1198
00:53:14,819 --> 00:53:16,680
encoding and we send it to the decoder

1199
00:53:16,680 --> 00:53:19,500
as decoder input the decoder will take

1200
00:53:19,500 --> 00:53:20,520
this

1201
00:53:20,520 --> 00:53:24,420
um his input as a query and the key and

1202
00:53:24,420 --> 00:53:27,660
the values coming from the encoder

1203
00:53:27,660 --> 00:53:30,119
and it will produce an output which is

1204
00:53:30,119 --> 00:53:33,180
sequenced by D model again we want the

1205
00:53:33,180 --> 00:53:35,160
linear layer to project it back to our

1206
00:53:35,160 --> 00:53:37,800
vocabulary and this projection is called

1207
00:53:37,800 --> 00:53:39,300
logits

1208
00:53:39,300 --> 00:53:41,819
what we do is we apply the soft Max

1209
00:53:41,819 --> 00:53:45,300
which will select given the logists will

1210
00:53:45,300 --> 00:53:48,720
give the position of the output word

1211
00:53:48,720 --> 00:53:51,480
will have the maximum score with the

1212
00:53:51,480 --> 00:53:54,480
soft Max this is how we know what words

1213
00:53:54,480 --> 00:53:57,180
to select from the vocabulary

1214
00:53:57,180 --> 00:53:59,760
and this hopefully should produce the

1215
00:53:59,760 --> 00:54:02,280
first output token which is T

1216
00:54:02,280 --> 00:54:05,660
if the model has been trained correctly

1217
00:54:05,660 --> 00:54:09,180
this however happens at time step one so

1218
00:54:09,180 --> 00:54:11,400
when we train the model Transformer

1219
00:54:11,400 --> 00:54:13,980
model it happens in one pass so we have

1220
00:54:13,980 --> 00:54:15,960
one input sequence one output sequence

1221
00:54:15,960 --> 00:54:17,819
we give it to the model we do it one

1222
00:54:17,819 --> 00:54:19,800
time step and the model will learn it

1223
00:54:19,800 --> 00:54:22,140
when we inference however we need to do

1224
00:54:22,140 --> 00:54:24,359
it token by token and we will also see

1225
00:54:24,359 --> 00:54:27,180
why this is the case

1226
00:54:27,180 --> 00:54:30,420
at time Step 2 we don't need to

1227
00:54:30,420 --> 00:54:34,079
recompute the encoder output again

1228
00:54:34,079 --> 00:54:37,740
because the over English sentence didn't

1229
00:54:37,740 --> 00:54:41,339
change so we hope the the encoder should

1230
00:54:41,339 --> 00:54:44,760
produce the same output for it and then

1231
00:54:44,760 --> 00:54:47,400
what we do is we take the output of the

1232
00:54:47,400 --> 00:54:49,559
previous sentence so

1233
00:54:49,559 --> 00:54:50,520
um

1234
00:54:50,520 --> 00:54:55,559
as T we append it to the input of the

1235
00:54:55,559 --> 00:54:57,900
decoder and then we feed it to the

1236
00:54:57,900 --> 00:55:01,020
decoder again with the output of the

1237
00:55:01,020 --> 00:55:03,180
encoder from the previous step

1238
00:55:03,180 --> 00:55:05,220
which will produce an output sequence

1239
00:55:05,220 --> 00:55:07,619
from the decoder side which we again

1240
00:55:07,619 --> 00:55:11,359
project back into our vocabulary and

1241
00:55:11,359 --> 00:55:15,300
we get the next token which is ammo

1242
00:55:15,300 --> 00:55:18,839
so as I saw before as I as I said before

1243
00:55:18,839 --> 00:55:21,900
we are not recalculating the output of

1244
00:55:21,900 --> 00:55:23,819
the encoder for every time step because

1245
00:55:23,819 --> 00:55:26,280
our English sentence didn't change at

1246
00:55:26,280 --> 00:55:28,800
all what is changing is the input of the

1247
00:55:28,800 --> 00:55:30,720
decoder because at every time step we

1248
00:55:30,720 --> 00:55:32,579
are appending the output of the previous

1249
00:55:32,579 --> 00:55:35,700
step to the input of the decoder we do

1250
00:55:35,700 --> 00:55:38,819
the same for the time step 3

1251
00:55:38,819 --> 00:55:41,420
and we do the same for the time step 4

1252
00:55:41,420 --> 00:55:44,940
and hopefully we will stop when we see

1253
00:55:44,940 --> 00:55:47,400
the end of sentence token because that

1254
00:55:47,400 --> 00:55:50,220
is that's how the model tells us to stop

1255
00:55:50,220 --> 00:55:51,720
inferencing

1256
00:55:51,720 --> 00:55:54,660
and this is how the inference works why

1257
00:55:54,660 --> 00:55:57,180
we needed four time steps

1258
00:55:57,180 --> 00:55:59,819
when we inference a model

1259
00:55:59,819 --> 00:56:02,040
um like the in this case the translation

1260
00:56:02,040 --> 00:56:04,500
model there are many strategies for

1261
00:56:04,500 --> 00:56:06,599
inferencing what we used is called

1262
00:56:06,599 --> 00:56:10,319
greedy strategy so for every step we get

1263
00:56:10,319 --> 00:56:14,059
the word with the maximum soft max value

1264
00:56:14,059 --> 00:56:18,660
and however this strategy Works uh

1265
00:56:18,660 --> 00:56:20,940
usually not bad but there are better

1266
00:56:20,940 --> 00:56:22,380
strategies

1267
00:56:22,380 --> 00:56:24,900
and one of them is called beam search

1268
00:56:24,900 --> 00:56:27,119
in beam search instead of always

1269
00:56:27,119 --> 00:56:29,640
greedily so this is that's why it's

1270
00:56:29,640 --> 00:56:31,800
called greedy instead of greedily taking

1271
00:56:31,800 --> 00:56:35,579
the maximum soft value we take the top B

1272
00:56:35,579 --> 00:56:39,540
values and then for each of these

1273
00:56:39,540 --> 00:56:42,660
choices we inference what are the next

1274
00:56:42,660 --> 00:56:46,140
possible tokens for each of the top B

1275
00:56:46,140 --> 00:56:49,920
values at every step and we keep only

1276
00:56:49,920 --> 00:56:52,559
the one with the B most probable

1277
00:56:52,559 --> 00:56:55,740
sequences and we delete the others this

1278
00:56:55,740 --> 00:56:58,140
is called beam search and it generally

1279
00:56:58,140 --> 00:57:00,660
it performs better

1280
00:57:00,660 --> 00:57:04,260
so thank you guys for watching uh I know

1281
00:57:04,260 --> 00:57:06,359
it was a long video but it was really

1282
00:57:06,359 --> 00:57:08,700
worth it to go through each aspect of

1283
00:57:08,700 --> 00:57:11,760
the Transformer I hope you enjoyed this

1284
00:57:11,760 --> 00:57:13,680
journey with me so please subscribe to

1285
00:57:13,680 --> 00:57:16,079
the channel and don't forget to watch my

1286
00:57:16,079 --> 00:57:18,599
other video on how to code a Transformer

1287
00:57:18,599 --> 00:57:20,700
model from scratch in which I describe

1288
00:57:20,700 --> 00:57:23,640
not only again the structure of the

1289
00:57:23,640 --> 00:57:26,460
Transformer model while coding it but I

1290
00:57:26,460 --> 00:57:30,300
also show you how to train it on a data

1291
00:57:30,300 --> 00:57:33,000
set of your choice how to inference it

1292
00:57:33,000 --> 00:57:35,940
and I also provided the code on GitHub

1293
00:57:35,940 --> 00:57:41,460
and the Ecolab notebook to train the

1294
00:57:41,460 --> 00:57:43,760
model directly on collab

1295
00:57:43,760 --> 00:57:46,619
please subscribe to the to the channel

1296
00:57:46,619 --> 00:57:48,420
and let me know what you didn't

1297
00:57:48,420 --> 00:57:51,180
understand so that I can give more

1298
00:57:51,180 --> 00:57:53,640
explanation and please tell me what are

1299
00:57:53,640 --> 00:57:55,740
the problems in this kind of videos or

1300
00:57:55,740 --> 00:57:57,480
in this particular video that I can

1301
00:57:57,480 --> 00:58:00,420
improve for the next videos thank you

1302
00:58:00,420 --> 00:58:03,359
very much and have a great rest of the

1303
00:58:03,359 --> 00:58:05,480
day

